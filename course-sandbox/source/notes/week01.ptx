<?xml version="1.0" encoding="UTF-8"?>

<pretext>
  <article xml:id="notes">
    <section xml:id="notes-week-01">
      <title>Week 1</title>

      <chapter xml:id="ch-math-340-instructor-guide">
        <title>Math 340 Instructor Guide</title>

        <p>
          <em>Fall 2025</em>
        </p>
      </chapter>


      <chapter xml:id="ch-how-to-use-this-guide">
        <title>How to use this guide</title>

        <p>
          This is the Math 340 Instructor Guide, organized by book chapter.
          <term>Please do not share this document with the students.</term> This is intended to help guide you in developing your lecture materials to ensure everyone is consistent with notation and course concepts when preparing for Midterms/the Final.
        </p>

        <p>
          The book organizes each chapter by: Discovery, Terminology, Concepts, Examples, and Theory.
          There are sometimes additional exercises at the end of the chapter.
          You should encourage your students to try the Discovery problems and read through the chapter up to Theory before class, and try out the exercises and discussion worksheets after class.
          In lecture, you‚Äôll likely want to use a bit of the ideas of the discovery section to get started, then intertwine the terminology, examples, concepts, and theory through the lecture.
          Make sure to cover the learning goals and pay attention to any additional notes given in the instructor guide.
        </p>

        <p>
          This document will also be shared with the teaching assistants to help them prepare for discussion.
          There are also notes to the teaching assistants with some advice for preparing for weekly discussion.
        </p>
      </chapter>


      <chapter xml:id="ch-general-information-about-math-340">
        <title>General Information about Math 340</title>

        <p>
          Math 340 is Elementary Matrix and Linear Algebra.
          It is a linear algebra course that focuses on the computational aspects of the subject.
          We have other introductory linear algebra courses (Math 341, Math 375) which focus on developing proof-writing skills.
          Additionally, we have a combined introductory differential equations/linear algebra course (Math 320).
          Students in Math 340 are typically computer science, data science, and math majors, with some graduate-level students from other STEM fields (they typically end up auditing).
        </p>

        <p>
          Math 340 has a prerequisite of Math 222, which is Calculus 2.
          They will have seen integration techniques, basic differential equations and Taylor series.
        </p>

        <section xml:id="sec-we-will-still-require-them-to-prove-verify-claims-but-in-a-way-that-s-more-systematic-than-a-theory-based-course-for-example-we-will-ask-students-to-prove-disprove-sets-are-subspaces-prove-disprove-functions-are-linear-transformations-and-we-will-often-require-them-to-explain-justify-their-claims-on-exams-lecture-time-should-be-a-balance-between-proving-theorems-and-applying-theorems-and-showing-examples-often-they-want-to-see-non-examples-of-things-ex-when-defining-invertible-they-want-to-see-a-non-invertible-matrix-right-away-they-do-appreciate-seeing-proofs-in-lecture-as-they-want-to-understand-why-things-work-and-they-know-they-ll-get-additional-examples-in-discussion-">
          <title>We will still require them to prove/verify claims, but in a way that's more systematic than a theory-based course. For example, we will ask students to prove/disprove sets are subspaces, prove/disprove functions are linear transformations, and we will often require them to explain/justify their claims on exams. Lecture time should be a balance between proving theorems and applying theorems and showing examples. Often they want to see non-examples of things (ex: when defining invertible, they want to see a non-invertible matrix right away). They DO appreciate seeing proofs in lecture, as they want to understand WHY things work, and they know they‚Äôll get additional examples in discussion.</title>

        </section>
      </chapter>


      <chapter xml:id="ch-first-day-of-class">
        <title>First Day of Class</title>

      </chapter>


      <chapter xml:id="ch-instructor-notes-on-the-first-day-of-class-you-should-briefly-go-over-course-policies-logistics-before-going-into-the-material-i-ve-made-a-short-slide-deck-for-you-but-if-you-choose-not-to-use-it-here-s-what-to-emphasize-">
        <title><em>Instructor notes:</em> On the first day of class, you should briefly go over course policies/logistics before going into the material. I‚Äôve made a short slide deck for you, but if you choose not to use it, here‚Äôs what to emphasize:</title>

        <p>
          -Go to Canvas, show them the syllabus and Course schedule
          <p>
            &#x26;nbsp;
          </p>
          -Show them each grade category
        </p>

        <p>
          <em>TA notes:</em> If you have discussion before the first day of lecture, you‚Äôll only do Worksheet 1. Worksheet 1 covers the basics of linear systems without setting up the augmented matrix, and it does introduce some terminology (ex: homogeneous, consistent/inconsistent) that will be defined in lecture in future class meetings.
        </p>

        <p>
          On the first day of discussion:
        </p>

        <p>
          <ol>
            <li>
              <p>
                Make sure to introduce yourself and tell students what you‚Äôd like to be called (and maybe even write it out phonetically on the board).
                Tell them how to contact you and how to find your office hours.
              </p>
            </li>

            <li>
              <p>
                Briefly go over the structure of discussion: Group work on worksheets, followed by a graded discussion activity at the end of discussion (either a 15-minute quiz or a 5-minute exit ticket).
                Tell them how to find the weekly discussion schedule (on Canvas).
                Tell them that the instructor will go over the course policies in lecture, and if they have questions they should post on Piazza.
              </p>
            </li>

            <li>
              <p>
                Put them in groups, have them spend five minutes introducing themselves and exchanging contact information, then tell them to start the worksheets.
              </p>
            </li>

            <li>
              <p>
                If they ask you course questions, encourage them to check the syllabus and/or post them on Piazza unless you‚Äôre completely sure you know the correct answer.
              </p>
            </li>

            <li>
              <p>
                Note: For the first few weeks of discussion, make sure to write your name on the board along with the discussion agenda for that day.
              </p>
            </li>
          </ol>
        </p>
      </chapter>


      <chapter xml:id="ch-week-1-chapters-1-and-2">
        <title>Week 1: Chapters 1 and 2</title>

        <p>
          <term>Chapter 1 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to solve linear systems using the method of elimination
              </p>
            </li>

            <li>
              <p>
                Be able to set up an augmented matrix for a linear system of equations AND be able to interpret an augmented matrix as a linear system of equations
              </p>
            </li>

            <li>
              <p>
                Be able to perform elementary row operations to a matrix
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 1 Terms:</term> Linear Equation, System of Linear Equations, Solution, Solution set, Consistent/Inconsistent system, parametric equation, matrix, augmented matrix, matrix entry, row, column, elementary row operations.
        </p>

        <p>
          <em>Instructor notes:</em> This book hits the ground running with solving linear systems. The goal is to get to Gaussian Elimination quickly, so it roughly defines matrices in order to get to setting up systems as matrices. It‚Äôs a good idea to reassure the students that we‚Äôll come back to this topic rigorously soon.
        </p>

        <p>
          <term>Chapter 2 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Know how to find a REF or the RREF form for a matrix
              </p>
            </li>

            <li>
              <p>
                Be able to solve linear system using the method of Gaussian Elimination
              </p>
            </li>

            <li>
              <p>
                Be able to find the rank of a matrix and know what it tells you about the number of solutions for the corresponding linear system
              </p>
            </li>

            <li>
              <p>
                Know how to calculate the number of parameters for a consistent system given the rank of the augmented matrix.
              </p>
            </li>

            <li>
              <p>
                Understand the difference between an augmented matrix and a coefficient matrix.
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 2 Terms:</term> row echelon form (REF), reduced row echelon form (RREF), leading one/pivot, row reduction, row-equivalent matrices, rank, leading variables, free variables, general solution, homogeneous system, coefficient matrix, trivial/nontrivial solution, pivot
        </p>

        <p>
          <em>Instructor notes:</em> The book doesn‚Äôt use the word ‚Äúpivot‚Äù to describe the leading ones, but I strongly encourage you to call it that, since it‚Äôs so prevalent in other resources. Many of the discussion worksheet solutions refer to ‚Äúpicking up the pivots‚Äù when we do column spaces later on.
        </p>

        <p>
          They get really fixated on the size of the matrix and what it tells you about the number of solutions.
          It‚Äôs good to emphasize that the size of the matrix doesn‚Äôt necessarily tell you how many solutions there are.
          Ex: square systems don‚Äôt necessarily have a unique solution.
          Discussing the rank should be helpful for these questions.
        </p>

        <p>
          About using the word ‚Äútrivial‚Äù (for homogeneous systems): it‚Äôs a necessary word, but they get really worked up about it, as some professors have said it insultingly in the past (ex: ‚ÄúThis exercise is trivial, so I‚Äôll let you work it out on your own‚Äù).
          Point out that you‚Äôre not saying something is easy or silly, but a defined notion.
        </p>

        <p>
          <em>TA Notes:</em> If showing row operations in discussion, PLEASE write out your row operations (ex: 3r1+r2r2), and try to match notation with your instructor. We expect the students to show their work, so it‚Äôs good to model it, as well. Additionally, please avoid encouraging non-elementary row operations and/or doing parallel row operations until later. The students should understand what an elementary row operation is in order to correctly apply determinant facts and build elementary matrices.
        </p>

        <p>
          As stated in the instructor notes, students often struggle with row-reducing to REF and RREF.
          When showing them examples in discussion, make sure to explain why you‚Äôre doing each step (ex: I want a leading one, so I‚Äôm swapping rows instead of dividing out by 4 to avoid fractions).
        </p>
      </chapter>


      <chapter xml:id="ch-week-2-chapters-2-and-4">
        <title>Week 2: Chapters 2 and 4</title>

        <p>
          <term>Chapter 4 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to pick out an element of a matrix given its indices
              </p>
            </li>

            <li>
              <p>
                Know what it means for two matrices to be equal vs.
                row equivalent
              </p>
            </li>

            <li>
              <p>
                Know how to perform matrix operations: sum/difference,  scalar multiplication, transpose, matrix-vector product (both as a linear combination and using dot product), matrix multiplication (both as repeated matrix-vector multiplication and using dot product)
              </p>
            </li>

            <li>
              <p>
                Know the basic properties of matrix operations and how they interact
              </p>
            </li>

            <li>
              <p>
                Know which sizes of matrices can be multiplied together, and what the resulting product matrix‚Äôs size will be
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 4 Terms:</term> (i,j)th entry of a matrix, size/dimension of a matrix, equal matrices, matrix addition, scalar multiple, linear combination, zero matrix, row vector, column vector, vector of unknowns, vector of constants, vector form, square matrix, main diagonal, transpose
        </p>

        <p>
          <em>Instructor notes:</em> You should do an example of linear combination in class when teaching Chapter 4. The book doesn‚Äôt fully describe the notion of a matrix-vector product as a linear combination. I would recommend that you write this out explicitly (they have a homework problem on this).
        </p>

        <p>
          Some students have seen matrix multiplication before, but don‚Äôt assume they‚Äôve all seen it.
          It‚Äôs important to give them both notions of matrix-vector multiplication and matrix-matrix multiplication to set up future material.
          For example, if they understand matrix-vector multiplication as a linear combination, this makes understanding the image of a matrix transformation very easy.
          If they understand matrix-matrix multiplication as repeated matrix-vector product, this makes things like diagonalization easier to understand.
        </p>

        <p>
          <em>TA Notes:</em> Please note that we are teaching them two equivalent notions of matrix-vector multiplication and two equivalent notions of matrix-matrix multiplication. This is not to be ‚Äúannoying‚Äù, but to help them better understand later ideas in the course (see instructor notes above).
        </p>
      </chapter>


      <chapter xml:id="ch-week-3-chapters-4-5-and-6">
        <title>Week 3: Chapters 4, 5, and 6</title>

        <p>
          <term>Chapter 5 Learning goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be familiar with properties of inverses of matrices: uniqueness, inverse of a product, inverse of inverse, inverse of transpose
              </p>
            </li>

            <li>
              <p>
                Know how to use matrix inverses to solve linear systems of equations
              </p>
            </li>

            <li>
              <p>
                Know the 2x2 inverse formula
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 5 terms:</term> Identity matrix, inverse, invertible matrix, singular matrix
        </p>

        <p>
          <em>Instructor Notes:</em> The book splits up Matrix inverses into two chapters: <term>Chapter 5</term> defines matrix inverses, gives the 2x2 inverse formula, and introduces properties of inverses. <term>Chapter 6</term> introduces elementary matrices and gives the algorithm for computing inverses [A:In ]->  [In:A-1].
        </p>

        <p>
          To start, it‚Äôs recommended that you give an example of a 2x2 matrix with an inverse and an example of a 2x2 matrix without an inverse.
          Connect this to the notion of inverting a nonzero number in the real numbers, but point out that there are more matrices than just the zero matrix which are noninvertible.
        </p>

        <p>
          When showing them how to use the matrix inverse to solve a linear system, show them multiplied examples with the same coefficient matrix to demonstrate that the only modification is the constant vector.
          In particular, show them the homogeneous case.
        </p>

        <p>
          When discussing the identity (AB)-1=B-1A-1, I use the analogy of putting on socks, then shoes.
          This is not commutative.
          When you want to ‚Äúundo‚Äù this action, what do you do first? What do you do second?
        </p>

        <p>
          <em>TA Notes:</em> Make sure to pay attention to what has been covered in lecture when going through the Chapter 5 and 6 worksheets. If the inverse algorithm hasn‚Äôt been covered yet, don‚Äôt use it in discussion.
        </p>

        <p>
          <term>Chapter 6 Learning goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Understand how to move between a row reduction and an elementary matrix
              </p>
            </li>

            <li>
              <p>
                Understand how to decompose a matrix into elementary matrices
              </p>
            </li>

            <li>
              <p>
                Be able to compute inverse matrices directly using row reduction
              </p>
            </li>

            <li>
              <p>
                Know the equivalent properties to determine if a matrix is invertible (see <url href="https://sites.ualberta.ca/~jsylvest/books/DLA/section-elem-matrices-theory.html#theorem-elem-matrices-equiv-to-invertible">Theorem 6.5.2: Characterizations of Invertibility</url> of your textbook, i.e.
                ‚ÄúBox of Facts‚Äù)-We will add to this
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 6 terms:</term> Elementary matrix
        </p>

        <p>
          <em>Instructor Notes:</em> We haven‚Äôt tested on Elementary matrices in previous semesters. We will likely test on them very lightly going forward; encourage them to do their homework assignments to prepare for the exam.
        </p>

        <p>
          If they‚Äôre getting confused about the matrix inverse algorithm, you can point out that another way to view it is as solving n linear systems simultaneously (maybe write it out for a 2x2 or 3x3 case).
        </p>

        <p>
          When going through the Characterizations of Invertibility, make sure to explain the logic: If one of these facts is true, then all the others are true.
          In other words, they imply each other.
          Spend a short amount of time discussing some of the justifications (ex: Why must invertibility imply that Ax=0 has only the trivial solution?)
        </p>
      </chapter>


      <chapter xml:id="ch-week-4-6-7-8">
        <title>Week 4: 6, 7, 8</title>

        <p>
          <term>Chapter 7 Learning goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be familiar with the special forms of square matrices (diagonal, scalar, upper/lower triangular, symmetric, and skew-symmetric) and their algebraic properties
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 7 terms:</term> Scalar matrix, diagonal matrix, upper triangular matrix, lower triangular matrix, symmetric matrix, skew-symmetric matrix
        </p>

        <p>
          <em>Instructor Notes:</em> The book defines skew-symmetric in the exercises, but you should define it in lecture. This section should take less than 50 minutes to get through. I would recommend picking one of the special forms from <url href="https://sites.ualberta.ca/~jsylvest/books/DLA/section-special-forms-theory.html#proposition-special-forms-combinations">Proposition 7.5.1</url> to prove in class. On exams/quizzes, we would ask them to prove something like this for a specific size matrix (like 3x3), so I‚Äôd recommend doing the same in lecture. This will allow you to write out explicitly the form of the matrix. In discussion, they will prove that symmetric matrices are closed under addition and scalar multiplication. Note that ‚Äúclosed‚Äù hasn‚Äôt been defined for them yet. If you choose to use that word in lecture, make sure to tell them what it means.
        </p>

        <p>
          <em>TA Notes:</em> The Chapter 7 content in lecture might be quick, but the worksheet will likely be challenging for them. For exercise 3, I‚Äôm envisioning having them prove it using coordinates instead of the A^T=A definition of symmetric matrices. The goal is to get them to practice this to set up proving/disproving sets are subspaces using their coordinates.
        </p>

        <p>
          <term>Chapter 8 Learning goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to compute determinants of small (2 √ó 2 , 3 √ó 3, and 4 √ó 4) matrices using cofactor expansion
              </p>
            </li>

            <li>
              <p>
                Be able to compute determinants of larger matrices using cofactor expansion through specific choice of row/column
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 8 terms:</term> minor, cofactor, cofactor expansion along a row/column, determinant
        </p>

        <p>
          <term>Warning:</term> Your students will want to use the ‚ÄúCriss-Cross‚Äù method of computing 3x3 determinants (formally called the Rule of Sarrus). You should tell them that while they can use it in practice, we‚Äôre using the cofactor expansion in class for many reasons: 1. It easily shows how row-operations affect determinants, and 2. This method doesn‚Äôt generalize to larger matrices. They should know how to compute determinants using cofactor expansion. They have a homework problem that forces them to write out the cofactor expansion for a 3x3 matrix.
        </p>

        <p>
          <em>Instructor Notes:</em> Determinants are split across three chapters: Chapter 8 motivates the idea of a determinant and teaches them how to compute the determinant via cofactor expansion. Chapter 9 discusses how row operations change the determinant. Chapter 10 gives the invertibility condition for determinants.
        </p>

        <p>
          They‚Äôve seen cross products before in Calculus 2, so they‚Äôre familiar with the process for 3x3 matrices, but some of them don‚Äôt realize that you can pick any row/column, and that it generalizes to bigger matrices.
        </p>

        <p>
          The discovery guide in the book does a great job of slowly building up to the 3x3 determinant.
          You likely won‚Äôt have time to do this fully in lecture.
          <p>
            &#x26;nbsp;
          </p>
          Here‚Äôs a suggested approach in lecture for setting up the cofactor expansion of the determinant:
          <p>
            &#x26;nbsp;
          </p>
          1.
          Recall the 2x2 inverse formula, and what conditions need to be satisfied for this to work (maybe derive the formula if you have time)
          <p>
            &#x26;nbsp;
          </p>
          2.
          Define the 2x2 determinant
          <p>
            &#x26;nbsp;
          </p>
          3.
          Give goal: Determine a number (called the determinant) that you compute for an nxn matrix that tells you whether or not the matrix is invertible.
          <p>
            &#x26;nbsp;
          </p>
          4.
          Jump into defining the minors, then cofactors, then determinant.
        </p>

        <p>
          Some instructors introduce determinants using the geometric meaning: This probably connects with some students, but a lot of them learned cross-product at the end of Calc 2 when they were fully checked out.
        </p>

        <p>
          <em>TA notes:</em> Make sure to read the warning above in case they have questions about using the criss-cross method. We may ask them to compute 4x4 determinants on quizzes/exams.
        </p>
      </chapter>


      <chapter xml:id="ch-">
        <title></title>

      </chapter>


      <chapter xml:id="ch-week-5-8-9-10-possibly-start-12">
        <title>Week 5: 8, 9, 10, possibly start 12</title>

        <p>
          <term>Chapter 9 Learning goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be familiar with the properties of determinants (Determinants of special forms, row-operations)
              </p>
            </li>

            <li>
              <p>
                Be able to compute determinants of larger matrices using properties of determinants
              </p>
            </li>
          </ul>
        </p>

        <p>
          <em>Instructor Notes:</em> I give a quick proof of many of the determinant properties: multiplying a row by a constant, det(A)=det(AT), having a row of zeros makes det(A)=0. You can likely have them participate and give you ideas on how to prove these. I show that adding a multiple to another row doesn‚Äôt change the determinant by showing it for 2x2. It‚Äôs also useful to point out at this stage that they should be careful about row operations: 3r1+r2r2 won‚Äôt change the determinant, but r1+3r2r2 will change it.
        </p>

        <p>
          <term>Chapter 10 Learning goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be familiar with the properties of determinants (Determinant of a product)
              </p>
            </li>

            <li>
              <p>
                Know the relationship between determinants and inverses
              </p>
            </li>

            <li>
              <p>
                Know the equivalent properties to determine if a matrix is invertible (see <url href="https://sites.ualberta.ca/~jsylvest/books/DLA/section-more-det-theory.html#theorem-more-det-equiv-to-invertible">Theorem 10.5.3: Characterizations of Invertibility</url> of your textbook, i.e.
                ‚ÄúBox of Facts‚Äù)-We will add to this
              </p>
            </li>
          </ul>
        </p>

        <p>
          <em>Instructor Notes:</em> We‚Äôre going to skip the adjoint formula and Cramer‚Äôs rule for homework and testing. You are welcome to mention it in class, but don‚Äôt spend much time on it. It‚Äôs likely that this section will take less than 50 minutes, so try to start Chapter 12 this week if possible.
        </p>

        <p>
          A way to justify that det(A)=0 iff A is singular that doesn‚Äôt need elementary matrices: Reduce A to its reduced echelon form, B.
          If it‚Äôs the identity, then since det(I) is not zero, A‚Äôs determinant is not zero.
          If it has a row of zeros, then det(B)=0, which means A‚Äôs determinant is also zero.
        </p>

        <p>
          Again, spend some time discussing the Characterizations of invertibility with your students.
          Ask them which implications are not clear to them at this point, and spend a few minutes discussing them.
          Also play around with rephrasing the false version of these statements.
          For example, if a matrix is not invertible, why must its rank not be equal to its size?
        </p>
      </chapter>


      <chapter xml:id="ch-">
        <title></title>

      </chapter>


      <chapter xml:id="ch-">
        <title></title>

      </chapter>


      <chapter xml:id="ch-week-6-12-13-14">
        <title>Week 6: 12, 13, 14</title>

        <p>
          Overall note for this week: This material is pretty straight-forward and is covered in Calculus 2, so don‚Äôt be surprised/offended if your lecture attendance is light this week.
          We start abstract vector spaces the week after Midterm 1, and that will definitely be more challenging.
        </p>

        <p>
          <term>Chapter 12 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Understand the geometric notions of vectors and their algebra (addition, negative, subtraction, scalar multiple)
              </p>
            </li>

            <li>
              <p>
                Know the algebraic rules for vectors in R^n
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 12 terms:</term> directed line segment, initial point, terminal point, components, vector, n-dimensional vector, n-dimensional space, zero vector, vector addition, negative (of a vector), vector subtraction, scalar multiple of a vector, parallel vector, linear combination, standard basis of R^n
        </p>

        <p>
          <term>Chapter 13 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to compute length of vectors in R^2 and R^3
              </p>
            </li>

            <li>
              <p>
                Be able to normalize a vector
              </p>
            </li>

            <li>
              <p>
                Be able to compute the standard inner product/dot product of vectors
              </p>
            </li>

            <li>
              <p>
                Be familiar with the basic algebra rules of the dot product
              </p>
            </li>

            <li>
              <p>
                Be familiar with the Cauchy‚ÄìBunyakovsky‚ÄìSchwarz (CBS) Inequality and its implications (uniqueness of angle measure, triangle inequality)
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 13 terms:</term> norm/length/magnitude, unit vector, normalization, distance, dot product, angle
        </p>

        <p>
          <em>Instructor Notes for Chapters 12 and 13:</em> Chapters 12 and 13 are combined in the homework set. Students should have seen all these topics (except Cauchy-Schwarz) in Calculus 2, so it‚Äôs recommended to move through the definitions and examples quickly.
        </p>

        <p>
          <term>Chapter 14 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Know how to determine if a pair of vectors is orthogonal using the dot product
              </p>
            </li>

            <li>
              <p>
                Know how to compute the orthogonal projection and what it geometrically represents
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 14 terms:</term> orthogonal vectors, orthogonal projection, vector component of a vector u orthogonal to a second vector
        </p>

        <p>
          <em>Instructor Notes:</em> The  Orthogonal projection should be a review from Calculus 2, but it‚Äôs a topic that they struggle with, so it‚Äôs suggested you go over it carefully. We‚Äôll revisit this at the end of the semester when we cover the Gram-Schmidt process for inner products. We‚Äôll skip the majority of this chapter (skip: normal vectors of lines in the plane, point-normal form, and cross product).
        </p>
      </chapter>


      <chapter xml:id="ch-week-7-16-17">
        <title>Week 7: 16, 17</title>

        <p>
          <term>Chapter 16 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                If given one of the 10 vector space properties, be able to verify if the property holds/does not hold for a potential vector space
              </p>
            </li>

            <li>
              <p>
                Be able to come up with a counterexample to show a set/operation is NOT a vector space or violates a vector space property
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 16 terms:</term> vector addition, scalar multiplication, closure/closed under addition/scalar multiplication, vector space, vector, zero vector, negative vector, vector subtraction, trivial vector space, R^n, Mmxn(R), P(R), P_n(R)
        </p>

        <p>
          <term>Notation:</term>
          <p>
            &#x26;nbsp;
          </p>
          M_mn, M_mxn, M_mxn(R), and R^mxn all represent mxn matrices.
          In past semesters, we used M_mn, so it‚Äôs on old exams.
          Our current book uses M_mxn(R).
        </p>

        <p>
          P and P(R) both represent the set of all polynomials with real coefficients in a single variable (either x or t).
        </p>

        <p>
          P_d and P_d(R) both represent the set of all polynomials with real coefficients in a single variable (either x or t) of degree at most d.
          Our current book uses P_n(R), but I‚Äôm going to keep using P_d(R) on exams to avoid confusion with dimension later on.
        </p>

        <p>
          <em>Instructor Notes:</em> For exams and quizzes, they won‚Äôt need to memorize all 10 properties of a vector space. We‚Äôd ask them to show a specific property is true/not true. They need to know the definition of subspace (and the subspace test). If we want them to prove a vector space axiom for a given operation, we‚Äôll give them the general axiom.
        </p>

        <p>
          The three general vector spaces we‚Äôll be working with in this class are n-dimensional space, mxn matrices, and univariate real polynomials (all polynomials and polynomials of degree at most d).
          We don‚Äôt typically test them on the space of functions.
        </p>

        <p>
          When introducing Chapter 16, after giving the vector space axioms, it would be good to start with proving that the 2x2 matrices are closed under addition and scalar multiplication and find the zero vector to show how proving closure works.
          This also starts to generalize the notion of a ‚Äúzero vector‚Äù to abstract vector spaces.
        </p>

        <p>
          You should also introduce the vector space of polynomials and of polynomials of degree at most d.
          Do not do the whole proof of Pd is a vector space in class.
          It‚Äôs SUPER boring.
          Do closure under addition (discuss why we need lower degree polynomials), and maybe the zero vector, and then ask your class one more property they‚Äôd like to see.
        </p>

        <p>
          The Chapter 16 homework has a ‚Äúnon-standard‚Äù vector addition/multiplication on ‚Ñù2.
          The Chapter 16 worksheet also has a ‚Äúnon-standard‚Äù example (V=positive reals, x+y=xy, cx=x^c) which is also introduced in <url href="https://sites.ualberta.ca/~jsylvest/books/DLA/section-abstract-vec-spaces-examples.html#subsection-abstract-vec-spaces-examples-weird">Subsection 16.5.1: Verifying axioms: the space of positive numbers</url> You‚Äôre welcome to discuss this one in class if they are struggling with the notion of generalizing addition and scalar multiplication.
          If the students ask why we need to consider vector spaces other than ‚Ñùn, you can point out that other vector spaces have other operations we like to use (like polynomials and matrices).
          I also mention Tropical Geometry.
        </p>

        <p>
          You should go through <url href="https://sites.ualberta.ca/~jsylvest/books/DLA/section-abstract-vec-spaces-theory.html#proposition-abstract-vec-spaces-basic-vec-props">Proposition 16.6.2</url> and prove one or two of them.
          These properties will set up the subspace test in Chapter 17.
        </p>

        <p>
          <term>Chapter 17 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to verify that a subset of a vector space is or is NOT a subspace (i.e.
                use the Subspace test)
              </p>
            </li>

            <li>
              <p>
                Find a spanning set for a subspace
              </p>
            </li>

            <li>
              <p>
                Be able to verify if a vector is in the span of other vectors
              </p>
            </li>

            <li>
              <p>
                Be able to determine if two spanning sets generate the same subspace
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 17 terms:</term> subspace, trivial subspace, linear combination, subspace generated by a set of vectors, spanning set/generating set, solution space of a homogeneous system
        </p>

        <p>
          <em>Instructor Notes:</em> They should be able to prove/disprove W is a <term>(nonempty!!)</term> subspace for V=‚Ñùn. Emphasize that to prove something is a subspace, they can‚Äôt use numbers. When showing a set is NOT a subspace, I‚Äôm pretty picky about the use of counterexamples. They really shouldn‚Äôt use variables unless it‚Äôs obviously not closed under addition/multiplication for ANY choice of number, otherwise they won‚Äôt receive full credit.
          <p>
            &#x26;nbsp;
          </p>
          Ex: The set (a,b) where a+b 0 isn‚Äôt a subspace, and if they claim that ra+rb&#x3C;0 for any negative r, I would not give full credit, since there are cases where ra+rb=0.
        </p>

        <p>
          The book doesn‚Äôt introduce the term ‚Äúnullspace‚Äù until Chapter 21; it currently calls this set the ‚Äúsolution space of a homogeneous system.‚Äù You‚Äôre welcome to call it both at this time, but the homework/worksheets will call it solution space for now.
          I recommend you walk through the proof that the solution space of a homogeneous system is a subspace in class; it‚Äôs a good example to demonstrate that we sometimes need to rely on the known structure of matrix algebra to prove a set is a subspace instead of resorting to the coordinates.
        </p>

        <p>
          The online homework does a good job of showing them examples/nonexamples of subspaces and having them build counterexamples, but it unfortunately cannot provide feedback on their subspace proofs.
          Encourage them to work through the book examples, and the very long worksheet for Chapter 17.
          Encourage them to post questions on Piazza.
        </p>
      </chapter>


      <chapter xml:id="ch-week-8-18-19-20">
        <title>Week 8: 18, 19, 20</title>

        <p>
          <term>Chapter 18 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to determine if a set of vectors is linearly independent via:a system of equations, the determinant of a matrix, or demonstrating a linear combination
              </p>
            </li>

            <li>
              <p>
                Know how span/linear independence connects to the invertibility criteria of matrices
              </p>
            </li>

            <li>
              <p>
                Know how to reduce a linearly dependent set to a linearly independent set with the same span
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 18 terms:</term> linearly dependent and linearly independent
        </p>

        <p>
          They often get linearly independent and linearly dependent mixed up.
          I recommend that as soon as you give the test/definition for linear dependence (nontrivial linear combination equalling the zero vector), you also give the equivalent definition; one of the vectors can be written as a linear combination of the others.
          Do an example in R^3, and draw a picture in R^3 to demonstrate what happens in this case, but warn them that we cannot rely fully on pictures to understand this concept.
        </p>

        <p>
          After discussing this, then give the definitions for linear independence (only trivial linear combination, cannot write them as linear combinations) and again give them a picture in R^3 to demonstrate.
        </p>

        <p>
          When doing examples in this section, to save time, do not show the steps of row reducing the augmented matrix.
          You should show them the end matrix to demonstrate what it looks like to have linearly independent vectors (each unknown coefficient corresponds to a pivot).
        </p>

        <p>
          When working out your first example in R^3, some of them will immediately notice that it‚Äôs equivalent to think about the determinant of a matrix.
          It‚Äôs important for you to be very careful with wording here, and be picky with their wording, too.
          They‚Äôre going to try to say that that matrix is linearly dependent, which is not true.
          Impress on them the difference, and do an example with maybe 4 2x2 matrices that is relatively simple (maybe even use the ‚Äústandard‚Äù matrices).
          Impress on them that it doesn‚Äôt matter if the 2x2 matrices are invertible or not.
        </p>

        <p>
          Make sure to go over the propositions in this section, even if you don‚Äôt prove them in detail (maybe do an example instead) so you can build up to basis and dimension using the previous theory.
        </p>

        <p>
          <term>Chapter 19 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Know how to show a set is or is NOT a basis for a vector space
              </p>
            </li>

            <li>
              <p>
                Know how to find a basis from a set of vectors
              </p>
            </li>

            <li>
              <p>
                Be able to find the coordinates of a vector with respect to an ordered basis
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 19 terms:</term> basis, ordered basis, coordinates of a vector relative to an ordered basis B, coordinate vector associated to a vector w relative to a basis B
        </p>

        <p>
          <term>Chapter 20 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to find the dimension of a vector space/subspace
              </p>
            </li>

            <li>
              <p>
                Know the dimensions of the common finite-dimensional vector spaces:‚Ñùn, ùïÑmn, Pd
              </p>
            </li>

            <li>
              <p>
                Know how to use the dimension of a vector space to justify if a set is a basis
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 20 terms:</term> finite-dimensional vector space, infinite-dimensional vector space, dimension of a finite-dimensional vector space
        </p>

        <p>
          You should prove that the set of all polynomials P is an infinite dimensional vector space in order to show them that infinite-dimensional vector spaces are possible, but we won‚Äôt work with them in our class.
        </p>
      </chapter>


      <chapter xml:id="ch-week-9-20-21-22">
        <title>Week 9: 20, 21, 22</title>

        <p>
          <term>Chapter 21 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Know how rank connects to dimension of row spaces/column spaces
              </p>
            </li>

            <li>
              <p>
                Know how to find a basis for the column space of a matrix (pick up the pivots!)
              </p>
            </li>

            <li>
              <p>
                Find a basis for the nullspace of a matrix
              </p>
            </li>

            <li>
              <p>
                Find a basis for the image space of a matrix
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 21 Learning Goals:</term> Column Space, Row Space, Null Space, Nullity
        </p>

        <p>
          Note that we avoid doing column operations.
          We tell them to find a basis for the row space by row-reducing and picking the non-zero rows.
          We tell them to find a basis for column space by row-reducing and picking the pivot columns from the original matrix.
          You could technically do column operations, but it‚Äôs unnecessary.
          The nice thing about ‚Äúpicking up the pivots‚Äù is that it creates a basis from the original spanning set.
        </p>

        <p>
          The book will delay calling the column space the ‚Äúimage space‚Äù until Chapter 42/43, since it hasn‚Äôt defined matrix transformation yet.
        </p>

        <p>
          A good working example is key here for getting through the theorems; I would try to lightly justify the theorems generally, but the examples will be key.
          See below.
        </p>

        <p>
          ![][image1]
        </p>

        <p>
          <term>Chapter 22 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Know how to compute the change matrix given two ordered bases for a vector space, and how to use it to find the coordinate vector
              </p>
            </li>

            <li>
              <p>
                Know the basic properties of transition matrices (composition of transition matrices, inversion of transition matrices)
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 22 terms:</term> transition matrix/change of basis matrix
        </p>

        <p>
          Remember that we haven‚Äôt gone through linear transformations yet, so you can‚Äôt explain transition matrices as a linear transformation.
          This is good, in a sense; we really want to focus on the transition matrix as a tool to convert from one basis to another.
        </p>

        <p>
          They may still be very resistant to the notion of using other bases besides the ‚Äústandard one‚Äù for each vector space.
        </p>
      </chapter>


      <chapter xml:id="ch-week-10-24-25-26">
        <title>Week 10: 24, 25, 26</title>

        <p>
          <term>Chapter 24 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Know how to verify if a vector is an eigenvector for a matrix
              </p>
            </li>

            <li>
              <p>
                Be able to compute the eigenvectors/eigenvalues (real eigenvalues only) for a matrix using the characteristic equation
              </p>
            </li>

            <li>
              <p>
                Be able to find a set of basis of eigenvectors for an eigenvalue of a matrix (i.e.
                a basis for the eigenspace)
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 24 terms:</term> eigenvector, eigenvalue, eigenspace, characteristic polynomial, characteristic equation
        </p>

        <p>
          We may ask them to find eigenvalues of 3x3 matrices, but we‚Äôll make sure the matrix can be partially factored through a ‚Äúsmart‚Äù choice of cofactor expansion.
        </p>

        <p>
          You don't need to worry about complex eigenvalues (for test/homework purposes).
          They should be aware they exist, though.
          We‚Äôre going to encourage the TAs to walk through the eigenvalue calculation process in discussion again after lecture to emphasize where the det(I-A) equation comes from.
        </p>

        <p>
          <em>TA Notes:</em> As stated above, it would be great to go over the characteristic equation det(I-A) construction with them again in discussion before discussing the Chapter 24 Worksheet.
        </p>

        <p>
          Note about Chapters 25 and 26: The homework and the worksheets are combined for these sections.
          You may want to combine them when discussing these topics in class.
        </p>

        <p>
          <term>Chapter 25 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to find the algebraic multiplicity and geometric multiplicity of an eigenvalue
              </p>
            </li>

            <li>
              <p>
                Be able to determine if a matrix is diagonalizable
              </p>
            </li>

            <li>
              <p>
                Be able to diagonalize a matrix (i.e.
                find P and D)
              </p>
            </li>

            <li>
              <p>
                Use a diagonalization to compute higher powers of a matrix
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 25 terms:</term> Similar matrices, transition matrix, diagonalizable, algebraic multiplicity, geometric multiplicity
        </p>

        <p>
          <term>Chapter 26 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Understand the difference between similarity and row equivalence
              </p>
            </li>

            <li>
              <p>
                Know how to use similarity to find shared properties of similar matrices
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 26 terms:</term> Similar matrices, similarity class
        </p>

        <p>
          Note that this book gives the following shared properties of similar matrices: nullity, rank, determinant, characteristic equation, eigenvalues, dimensions of eigenspaces.
          They will do the determinant proof in discussion, but you can also do the characteristic polynomial/eigenvalues proof in class.
        </p>

        <p>
          The book doesn‚Äôt go over trace until chapter 46, but they mention in Chapter 46 that the trace was covered earlier; I think this is an oversight by the author.
          I can ask him to fix it.
        </p>

        <p>
          The book mentions that much of the theory can be generalized to block-diagonal forms, Jordan form, etc, but we will not go into that in this course.
        </p>
      </chapter>


      <chapter xml:id="ch-week-11-42-43-44">
        <title>Week 11: 42, 43, 44</title>

        <p>
          <term>Chapter 42 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to show a function between vector spaces is or is NOT a linear transformation
              </p>
            </li>

            <li>
              <p>
                Find the matrix representation of a linear transformation T:R^n->R^m
              </p>
            </li>

            <li>
              <p>
                Be able to determine if a vector is in the image of a linear transformation
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 42 terms:</term> matrix transformation, linear transformation, vector space homomorphism, domain space, codomain space, linearity properties, image vector, linear operator, standard matrix of a linear transformation, zero transformation, zero operator, identity operator, scalar operator, coordinate transformation
        </p>

        <p>
          Students should be able to show that a function is/is not a linear transformation on exams (i.e., use variables to show it‚Äôs a linear transformation, use explicit counterexamples to show what has failed).
          You can skip the discussion of complex space.
          We‚Äôre not going to go into the dual vector space in this course, so you can skip that part of the textbook (i.e.
          don‚Äôt go over <url href="https://sites.ualberta.ca/~jsylvest/books/DLA/section-lintrans-basic-theory.html#subsection-lintrans-basic-theory-hom-space">Subsection 42.5.2: Spaces of linear transformations</url>)
        </p>

        <p>
          Suggest approach for Chapter 42:
        </p>

        <p>
          <ol>
            <li>
              <p>
                Give definition of Linear transformation
              </p>
            </li>

            <li>
              <p>
                Give an example of a simple one from R^2 to R^2
              </p>
            </li>

            <li>
              <p>
                Realize it as a matrix transformation
              </p>
            </li>

            <li>
              <p>
                Show that every matrix transformation is a linear transformation
              </p>
            </li>

            <li>
              <p>
                Before they get too complacent, show them a T:P_2\to P_1 by T(p)=p‚Äô.
                Is this a linear transformation? (yes) Is it obviously a matrix transformation? (no, but soon we‚Äôll have a way to represent it using a matrix)
              </p>
            </li>
          </ol>
        </p>

        <p>
          <term>Chapter 43 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Know how to find the kernel and/or range/image of a matrix transformation (as well as a basis for the kernel/range)
              </p>
            </li>

            <li>
              <p>
                Know how to find vectors in the kernel/range of a general linear transformation
              </p>
            </li>

            <li>
              <p>
                Know the dimension theorem and how to use it to calculate possible dimensions of kernel and image.
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 43 terms:</term> kernel of a linear transformation, nullity, image, range/image, rank of a linear transformation
        </p>

        <p>
          The book delays mentioning surjective/injective functions (in connection to kernel/image) until Chapter 44, and focuses on finding kernels and images in this section.
          On an exam, we‚Äôd likely ask them to find the kernel/image using a matrix representation, but it‚Äôs good for them to practice this concept without the matrix representation now so they understand where each subspace ‚Äúlives‚Äù.
        </p>

        <p>
          <term>Chapter 44 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Know how to determine if a linear transformation is one-to-one and/or onto
              </p>
            </li>

            <li>
              <p>
                Know how the dimension of vector spaces for a linear transformation interacts with the problem of being one-to-one/onto
              </p>
            </li>

            <li>
              <p>
                Be able to determine if a linear transformation is an isomorphism
              </p>
            </li>

            <li>
              <p>
                Be able to build an isomorphism between two vector spaces
              </p>
            </li>

            <li>
              <p>
                Know how to determine if two finite-dimensional vector spaces are isomorphic
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 44 terms:</term> composite function, injective/one-to-one function, invertible transformation, inverse transformation, surjective/onto function, isomorphism, isomorphic spaces
        </p>

        <p>
          Careful: A common misconception by students is that all linear transformations between isomorphic vector spaces are automatically isomorphisms.
          Give some examples in class of when this fails, for example:
        </p>

        <p>
          ![][image2]
        </p>
      </chapter>


      <chapter xml:id="ch-week-12-45-46-and-catch-up-">
        <title>Week 12: 45, 46 (and catch-up)</title>

        <p>
          <term>Chapter 45 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Know how to find the matrix representing a linear transformation L : V ‚Üí W using ordered bases for vector spaces V and W
              </p>
            </li>

            <li>
              <p>
                Be able to find kernel/image of a linear transformation using a matrix representation
              </p>
            </li>

            <li>
              <p>
                Know how to verify if a linear transformation is invertible/an isomorphism using a matrix representation
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 45 terms:</term> matrix of a linear transformation
        </p>

        <p>
          Building the matrix representation of a linear transformation is‚Ä¶stressful for them.
          I teach it as a 3-step process: 1.
          Apply the linear transformation to each of the basis elements 2.
          Write each image in its coordinate representation 3.
          Put the columns in a matrix, following the same order as the original basis.
        </p>

        <p>
          One way to ‚Äúcheck‚Äù you set it up right is to apply the matrix representation to the first element in the ordered basis.
          If they do it right, it should ‚Äúpick up‚Äù the coordinates of the image of that vector using the other basis.
          I have extra examples for Chapters 45 and 46 in the Folder in Drive.
          Feel free to share those with them directly (just upload to your Canvas course), or go through them in class if you have extra time.
          They surprisingly really like the commutative diagrams.
        </p>

        <p>
          A really good example to do right away is T:P_2\to P_1 by T(p)=p‚Äô.
          Do it once with the ‚Äústandard‚Äù bases for P_2 and P_1, then switch to a nonstandard basis for each: B=(x^2+1,4, x+2) and D=(1,x), then use each to compute T(p) for a specific polynomial p.
          Remind them they have to ‚Äúput it back‚Äù into a polynomial to finish the computation.
          You can also use either of them to compute the kernel of T.
        </p>

        <p>
          <term>Chapter 46 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Know how to change the representation of a linear operator from one basis to another, both directly and using transition matrices (i.e., the Similarity Theorem)
              </p>
            </li>

            <li>
              <p>
                Be able to find the eigenvalues/eigenvectors of a linear operator
              </p>
            </li>

            <li>
              <p>
                Be able to find a diagonal matrix representation of a linear operator (if possible)
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 46 terms: determinant (of a linear operator), trace (of a linear operator), eigenvector (of a linear operator), eigenvalue (of a linear operator), eigenspace (of a linear operator), characteristic polynomial (of a linear operator), characteristic equation (of a linear operator), diagonalizable operator</term>
        </p>

        <p>
          The book doesn‚Äôt state the Similarity Theorem explicitly, but you should.
          With the Similarity theorem, it‚Äôs helpful to show the result using both change matrices instead of the inverse of the change matrix:
        </p>

        <p>
          MB(T)=PBDMD(T)PDB
        </p>

        <p>
          At that moment, they do get more confused, but here‚Äôs the analogy I use: suppose a student in a Spanish course is given an essay assignment, with a prompt in Spanish (B).
          If the student wants to write the essay in English (D), what do they do?
        </p>

        <p>
          <ul>
            <li>
              <p>
                Take the Spanish prompt (which is our input vector), and translate the prompt to English (that‚Äôs multiplying by PDB).
              </p>
            </li>

            <li>
              <p>
                Write the whole essay in English (that‚Äôs multiplying by MD(T))
              </p>
            </li>

            <li>
              <p>
                Translate the essay back to Spanish (that‚Äôs multiplying by PBD)
              </p>
            </li>
          </ul>
        </p>

        <p>
          Of course, that‚Äôs a terrible idea in reality, both for integrity reasons AND because Spanish/English doesn‚Äôt translate perfectly, BUT mathematically it does work perfectly :)
        </p>
      </chapter>


      <chapter xml:id="ch-week-13-36-and-catch-up-">
        <title>Week 13: 36 (and catch-up)</title>

        <p>
          If you‚Äôre teaching Tuesday/Thursday, you will only meet this week on Tuesday.
          You can really stretch out Chapter 36 and give them time to try out verifying inner product properties in lecture.
          If you‚Äôre teaching Monday/Wednesday, it‚Äôs likely that very few students will show up on Wednesday.
          I suggest trying to cover the important concepts/examples on Monday, and use Wednesday either for additional examples or teach them something you find interesting (like Page Rank, Markov chains, etc).
          You can NOT cancel Wednesday class (sorry).
        </p>

        <p>
          <term>Chapter 36 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Know how to verify if a function is/is not an inner product
              </p>
            </li>

            <li>
              <p>
                Be able to compute the length of a vector and find a unit vector given an inner product
              </p>
            </li>

            <li>
              <p>
                Be able to find the distance between two vectors using an inner product
              </p>
            </li>

            <li>
              <p>
                Be able to find the angle between two vectors using an inner product
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 36 terms:</term> pairing, (real) inner product, (real) inner product space, norm, unit vector, angle, positive definite real matrix
        </p>

        <p>
          You should start with a brief review of dot product, then again discuss with them the notion of ‚Äúabstractifying‚Äù the ideas of length, distance, orthogonality, etc to different vector spaces.
          <p>
            &#x26;nbsp;
          </p>
          Note that this book does not list ‚Äú&#x3C;v,v>=0 iff v=0‚Äù as one of the required inner product axioms..
          They should be able to verify a function is/is not an inner product, though we‚Äôll give them a specific property to prove/disprove.
          You should plan to go over the inner product &#x3C;p,q>=01p(x)q(x)dx (for polynomials) with them in class, and practice computing length and distance.
          They will try to use the dot product for everything.
        </p>

        <p>
          There should be room in the schedule to discuss additional inner products, such as &#x3C;A,B>=tr(ABT) for matrices, evaluation of polynomials, and defining an inner product in terms of a positive definite matrix.
        </p>

        <p>
          <term>Chapter 37 Learning Goals:</term>
        </p>

        <p>
          <ul>
            <li>
              <p>
                Know how to check if two vectors are orthogonal given an inner product
              </p>
            </li>

            <li>
              <p>
                Know how to find the coefficients of a linear combination using an orthogonal/orthonormal basis
              </p>
            </li>

            <li>
              <p>
                Be able to build an orthonormal basis given an orthogonal basis
              </p>
            </li>

            <li>
              <p>
                Be able to build an orthogonal basis given any basis using the Gram-Schmidt algorithm (note: not on final exam)
              </p>
            </li>
          </ul>
        </p>

        <p>
          <term>Chapter 37 terms:</term> orthogonal vectors, orthogonal complement, orthogonal set of vectors, orthonormal set of vectors
        </p>

        <p>
          Note: They should be prepared to check orthogonality for non-standard inner products.
          They should be able to use an inner product to find coefficients, and be able to build an orthonormal basis from an orthogonal basis.
          Gram-Schmidt is on the homework, but not on the final, so I‚Äôd go over it quickly in class (note: our book doesn‚Äôt do projections until later).
        </p>

        <p>
          We do often ask them to find a basis for an orthogonal complement on the final; you don‚Äôt need to get too fancy with this in class, just teach it as solving a system of (homogeneous) equations.
        </p>

        <p>
          On the Chapter 14 worksheet, they worked through a problem to discover that the coefficients of a linear combination of an orthonormal basis come from the inner product.
          You can use that example in class.
        </p>

        <p>
          <em>TA Notes:</em> Please emphasize computing inner products, length, and distance using weird inner products does NOT give the same answer as the dot product. Vectors which are orthogonal with respect to the dot product may not be orthogonal with respect to another inner product.
        </p>
      </chapter>


      <chapter xml:id="ch-">
        <title></title>

      </chapter>


      <chapter xml:id="ch-chapter-3-3-1-3-3-">
        <title>Chapter 3 (¬ß3.1-¬ß3.3)</title>

        <p>
          Learning Goals:
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to compute determinants of small (2 √ó 2 and 3 √ó 3) matrices using cofactor expansion
              </p>
            </li>

            <li>
              <p>
                Be familiar with the properties of determinants
              </p>
            </li>

            <li>
              <p>
                Be able to compute determinants of larger matrices using properties of determinants
              </p>
            </li>

            <li>
              <p>
                Be able to compute determinants of larger matrices using cofactor expansion through specific choice of row/column
              </p>
            </li>

            <li>
              <p>
                Know the relationship between determinants and inverses
              </p>
            </li>

            <li>
              <p>
                Know how to verify if a vector is an eigenvector for a matrix
              </p>
            </li>

            <li>
              <p>
                Be able to compute the eigenvectors/eigenvalues (real eigenvalues only) for a matrix using the characteristic equation
              </p>
            </li>

            <li>
              <p>
                Be able to find a set of basic eigenvectors for an eigenvalue of a matrix
              </p>
            </li>

            <li>
              <p>
                Be able to diagonalize a matrix
              </p>
            </li>

            <li>
              <p>
                Use a diagonalization to compute higher powers of a matrix
              </p>
            </li>
          </ul>
        </p>

        <p>
          <em>Instructor Notes:</em> We cover determinants using cofactor expansion. They‚Äôve seen cross products before in Calculus 2, so they‚Äôre familiar with the process for 3x3 matrices, but some of them don‚Äôt realize that you can pick any row/column, and that it generalizes to bigger matrices. We don‚Äôt make them compute determinants for 4x4 or bigger matrices via cofactor expansion.   I give a quick proof of many of the determinant properties: multiplying a row by a constant, det(A)=det(AT), having a row of zeros makes det(A)=0. I show that adding a multiple to another row doesn‚Äôt change the determinant by showing it for 2x2. It‚Äôs also useful to point out at this stage that they should be careful about row operations: 3r1+r2r2 won‚Äôt change the determinant, but r1+3r2r2 will change it.
        </p>

        <p>
          A way to justify that det(A)=0 iff A is singular that avoids elementary matrices: Reduce A to its reduced echelon form, B.
          If it‚Äôs the identity, then since det(I) is not zero, A‚Äôs determinant is not zero.
          If it has a row of zeros, then det(B)=0, which means A‚Äôs determinant is also zero.
          We skip Cramer‚Äôs rule and the adjoint matrix formula for inverses.
        </p>

        <p>
          You don't need to worry about complex eigenvalues (for test/homework purposes).
          They should be aware they exist, though.
          We‚Äôre going to encourage the TAs to walk through the eigenvalue calculation process in discussion again after lecture to emphasize where the det(I-A) equation comes from.
        </p>

        <p>
          We introduce diagonalization here, but we‚Äôll return to it in 5.5 once we have the correct terminology (linear independence and dimension).
          The homework for ¬ß3.3 is split into two parts:  Part 1 (Eigenvalues and Eigenvectors) is on Midterm 1, while Part 2 (Diagonalization) will be on Midterm 2.
          Please cover: the application of diagonalization for higher powers of A, and where P,D, P^(-1) come from (the eigenvectors/eigenvalues of A).
        </p>

        <p>
          <em>TA Notes:</em> As stated above, it would be great to go over the characteristic equation det(I-A) construction with them again in discussion before handing out the 3.3 worksheet. Your instructor may not get to diagonalization, so be aware of that if it‚Äôs on the 3.3 worksheet.
        </p>
      </chapter>


      <chapter xml:id="ch-">
        <title></title>

      </chapter>


      <chapter xml:id="ch-chapter-5-5-1-5-2-5-4-5-5-">
        <title>Chapter 5 (¬ß5.1-¬ß5.2, ¬ß5.4-¬ß5.5)</title>

        <p>
          Learning Goals:
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to verify that a subset of a vector space is or is NOT a subspace
              </p>
            </li>

            <li>
              <p>
                Find a spanning set for a subspace
              </p>
            </li>

            <li>
              <p>
                Be able to verify if a vector is in the span of other vectors
              </p>
            </li>

            <li>
              <p>
                Find a spanning set for the nullspace of a matrix
              </p>
            </li>

            <li>
              <p>
                Find a spanning set for the image space of  matrix
              </p>
            </li>

            <li>
              <p>
                Be able to determine if a set of vectors is linearly independent via:a system of equations, the determinant of a matrix, or demonstrating a linear combination
              </p>
            </li>

            <li>
              <p>
                Know how span/linear independence connects to the invertibility criteria of matrices
              </p>
            </li>

            <li>
              <p>
                Know how to show a set is or is NOT a basis for a subspace
              </p>
            </li>

            <li>
              <p>
                Know how to find the dimension of a subspace
              </p>
            </li>

            <li>
              <p>
                Know how rank connects to dimension of row spaces/column spaces
              </p>
            </li>

            <li>
              <p>
                Know how to find a basis for the column space of a matrix (pick up the pivots!)
              </p>
            </li>

            <li>
              <p>
                Understand the difference between similarity and row equivalence
              </p>
            </li>

            <li>
              <p>
                Know how to use similarity to find shared properties of similar matrices
              </p>
            </li>

            <li>
              <p>
                Be able to find the algebraic multiplicity and geometric multiplicity of an eigenvalue
              </p>
            </li>

            <li>
              <p>
                Be able to determine if a matrix is diagonalizable
              </p>
            </li>

            <li>
              <p>
                Be able to diagonalize a matrix (i.e.
                find P and D)
              </p>
            </li>
          </ul>
        </p>

        <p>
          <em>Instructor Notes:</em> The book introduces vector spaces/subspaces using ‚Ñùn first, then does the same thing again with general vector spaces.  They haven‚Äôt had set theory, so short-hand notation (is an element of, for all) is confusing, but the book does a nice job explaining this notation in footnotes. Also, the idea of subset is confusing, so it‚Äôs helpful to point out when a set is a strict subset of ‚Ñùn.
          <p>
            &#x26;nbsp;
          </p>
          They should be able to prove/disprove W is a <term>(nonempty!!)</term> subspace for V=‚Ñùn.
          Emphasize that to prove something is a subspace, they can‚Äôt use numbers.
          When showing a set is NOT a subspace, I‚Äôm pretty picky about the use of counterexamples.
          They really shouldn‚Äôt use variables unless it‚Äôs obviously not closed under addition/multiplication for ANY choice of number, otherwise they won‚Äôt receive full credit.
          <p>
            &#x26;nbsp;
          </p>
          Ex: The set (a,b) where a+b 0 isn‚Äôt a subspace, and if they claim that ra+rb&#x3C;0 for any negative r, I would not give full credit, since there are cases where ra+rb=0.
        </p>

        <p>
          When testing for linear independence, to save time, I would carefully set up the matrices to check for linear independence, but skip the row reductions.
          Note that we skip ¬ß5.3 for now (it‚Äôs on orthogonality, so we cover it right before Chapter 10).
        </p>

        <p>
          Note that we avoid doing column operations.
          We tell them to find a basis for the row space by row-reducing and picking the non-zero rows.
          We tell them to find a basis for column space by row-reducing and picking the pivot columns from the original matrix.
          You could technically do column operations, but it‚Äôs unnecessary.
          The nice thing about ‚Äúpicking up the pivots‚Äù is that it creates a basis from the original spanning set.
        </p>

        <p>
          We return to eigenvalues/eigenvectors in ¬ß5.5, and also define similarity.
          Diagonalization is now precisely given in terms of algebraic/geometric multiplicity.
        </p>

        <p>
          <em>TA Notes:</em> Students often confuse similarity with row-equivalence. Please stress the following:
          <p>
            &#x26;nbsp;
          </p>
          Similar is NOT the same thing as row-equivalent.
          <p>
            &#x26;nbsp;
          </p>
          Diagonalizable is NOT the same thing as invertible.
        </p>
      </chapter>


      <chapter xml:id="ch-">
        <title></title>

      </chapter>


      <chapter xml:id="ch-chapter-6-6-1-6-4-">
        <title>Chapter 6 (¬ß6.1-¬ß6.4)</title>

        <p>
          Learning Goals:
        </p>

        <p>
          <ul>
            <li>
              <p>
                If given one of the 10 vector space properties, be able to verify if the property holds/does not hold for a potential vector space
              </p>
            </li>

            <li>
              <p>
                Be able to come up with a counterexample to show a set/operation is NOT a vector space or violates a vector space property
              </p>
            </li>

            <li>
              <p>
                Be able to verify that a subset of a vector space is or is NOT a subspace
              </p>
            </li>

            <li>
              <p>
                Be able to determine if a vector is in the span of a set of vectors
              </p>
            </li>

            <li>
              <p>
                Be able to determine if a set spans a vector space
              </p>
            </li>

            <li>
              <p>
                Be able to determine if a set is linearly independent/linearly dependent (via systems of equations, the determinant of a matrix, or demonstrating a linear combination)
              </p>
            </li>

            <li>
              <p>
                Know how to show a set is or is NOT a basis for a vector space
              </p>
            </li>

            <li>
              <p>
                Know how to find a basis from a set of vectors
              </p>
            </li>

            <li>
              <p>
                Be able to find the dimension of a vector space/subspace
              </p>
            </li>

            <li>
              <p>
                Know the dimensions of the common finite-dimensional vector spaces:‚Ñùn, ùïÑmn, Pd
              </p>
            </li>

            <li>
              <p>
                Know how to use the dimension of a vector space to justify if a set is a basis
              </p>
            </li>
          </ul>
        </p>

        <p>
          <em>Instructor Notes:</em> On an exam, they won‚Äôt need to memorize all 10 properties of a vector space. We‚Äôd ask them to show a specific property is true/not true. They need to know the definition of subspace (and the subspace test). If we want them to prove a vector space axiom for a given operation, we‚Äôll give them the general axiom. If you want to teach them the ‚Äúfaster‚Äù subspace test (show a<term>x</term>+b<term>y</term> is in the set), that‚Äôs fine. Again, emphasize that to show a subset IS a subspace, they need to use variables (and <term>show nonempty</term>). To show a subset is NOT a subspace, they should use numbers for their counterexample.
        </p>

        <p>
          If using my notes for ¬ß6.1: Do not do the whole proof of P2 is a vector space in class.
          It‚Äôs SUPER boring.
          Do closure under addition, and maybe the zero vector, and then ask your class one more property they‚Äôd like to see.
        </p>

        <p>
          The ¬ß6.1 homework has a ‚Äúnon-standard‚Äù vector addition/multiplication on ‚Ñù2.
          The ¬ß6.1 worksheet also has a ‚Äúnon-standard‚Äù example (V=positive reals, x+y=xy, cx=x^c).
          If the students ask why we need to consider vector spaces other than ‚Ñùn, you can point out that other vector spaces have other operations we like to use (like polynomials and matrices).
          I also mention Tropical Geometry.
        </p>

        <p>
          ¬ß6.4 can be covered quickly if you‚Äôre running out of time.
          The main goal is to explicitly summarize/state a lot of the facts about finite vector spaces we‚Äôve been assuming.
        </p>

        <p>
          <em>TA Notes:</em> Again, when doing subspaces, PLEASE emphasize that to show something IS a subspace, you need to show nonempty, and need to show closure over addition/scalar multiplication using variables. To show it‚Äôs NOT a subspace, it‚Äôs necessary to use explicit counterexamples.
        </p>
      </chapter>


      <chapter xml:id="ch-chapter-7-midterm-2-material-7-1-7-2-">
        <title>Chapter 7 (Midterm 2 Material, ¬ß7.1-¬ß7.2)</title>

        <p>
          Learning Goals:
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to show a function between vector spaces is or is NOT a linear transformation
              </p>
            </li>

            <li>
              <p>
                Know how to determine if a linear transformation is one-to-one and/or onto
              </p>
            </li>

            <li>
              <p>
                Know how to find the kernel and/or range/image of a matrix transformation (as well as a basis for the kernel/range)
              </p>
            </li>

            <li>
              <p>
                Know how to find vectors in the kernel/range of a general linear transformation
              </p>
            </li>

            <li>
              <p>
                Know how the dimension of vector spaces for a linear transformation interacts with the problem of being one-to-one/onto
              </p>
            </li>
          </ul>
        </p>

        <p>
          <em>Instructor Notes:</em> Just like with subspaces, they should be able to prove a function is a linear transformation using variables, not numbers. They should also be able to show a function is NOT a linear transformation using explicit counterexamples.
        </p>

        <p>
          For Midterm 2, they should be able to find a basis for the kernel/image of a matrix transformation, and find elements of the kernel for general linear transformations.
          They should also be able to determine if vectors are in the image of a linear transformation.
          You should also emphasize the idea of moving between different dimension vector spaces and seeing if it‚Äôs possible to be one-to-one/onto.
        </p>
      </chapter>


      <chapter xml:id="ch-">
        <title></title>

      </chapter>


      <chapter xml:id="ch-chapter-7-final-exam-material-7-3-">
        <title>Chapter 7 (Final Exam Material, ¬ß7.3)</title>

        <p>
          Learning Goals:
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to determine if a linear transformation is an isomorphism
              </p>
            </li>

            <li>
              <p>
                Know how to determine if two finite-dimensional vector spaces are isomorphic
              </p>
            </li>
          </ul>
        </p>

        <p>
          <em>Instructor Notes:</em> Due to the order of this textbook, we can state the definition of isomorphism cleanly: it‚Äôs a linear transformation that is both one-to-one and onto. They should be able to justify if a linear transformation is an isomorphism. We‚Äôll go over the coordinate representation of a vector using an ordered basis more carefully in ¬ß9.1, so feel free to go over this quickly.
        </p>

        <p>
          ¬ß7.3 leads nicely into Chapter 9, as they find the idea of finding inverses of general isomorphisms SUPER annoying, so it‚Äôs nice to be able to switch to matrices and find an inverse that way.
        </p>

        <p>
          Students should also always ‚Äúdeclare‚Äù their isomorphism if they‚Äôre using ‚Ñùn to represent other vector spaces.
          For example, if they‚Äôre verifying that 3 polynomials in P2 are linearly independent and immediately write them as column vectors, they will not get full credit.
        </p>

        <p>
          <em>TA Notes:</em> As stated above, please emphasize that they should ‚Äúdeclare‚Äù their isomorphism if using column vectors to represent other vector spaces.
        </p>
      </chapter>


      <chapter xml:id="ch-">
        <title></title>

      </chapter>


      <chapter xml:id="ch-chapter-9-9-1-9-3-">
        <title>Chapter 9 (¬ß9.1-¬ß9.3)</title>

        <p>
          Learning Goals:
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to give the coordinate vector with respect to an ordered basis
              </p>
            </li>

            <li>
              <p>
                Know how to find the matrix representing a linear transformation L : V ‚Üí W using ordered bases for vector spaces V and W
              </p>
            </li>

            <li>
              <p>
                Know how to verify if a linear transformation is invertible/an isomorphism using matrices
              </p>
            </li>

            <li>
              <p>
                Know how to compute the change matrix given two ordered bases for a vector space, and how to use it to find the coordinate vector
              </p>
            </li>

            <li>
              <p>
                Know how to change the representation of a linear operator from one basis to another, both directly and using the Similarity Theorem (pg.
                507)
              </p>
            </li>

            <li>
              <p>
                Be able to find the eigenvalues/eigenvectors of a linear operator
              </p>
            </li>

            <li>
              <p>
                Be able to find a diagonal matrix representation of a linear operator (if possible)
              </p>
            </li>
          </ul>
        </p>

        <p>
          <em>Instructor Notes:</em>  Please note the notation used for coordinate vectors. We also call transition matrices ‚Äúchange matrices‚Äù following the book notation. They surprisingly really like the commutative diagrams. The book doesn‚Äôt show one for the Similarity Theorem, so I would give one (the one in my notes is a slight abuse of notation).
        </p>

        <p>
          Building the matrix representation of a linear transformation is‚Ä¶stressful for them.
          One way to ‚Äúcheck‚Äù you set it up right is to apply the matrix representation to the first element in the ordered basis.
          If they do it right, it should ‚Äúpick up‚Äù the coordinates of the image of that vector using the other basis.
          I have extra examples for 9.1 and 9.2 in the Completed Notes Folder in Drive.
          Feel free to share those with them directly (just upload to your Canvas course), or go through them in class if you have extra time.
        </p>

        <p>
          With the similarity theorem, it‚Äôs helpful to show the result using both change matrices instead of the inverse of the change matrix:
        </p>

        <p>
          MB(T)=PBDMD(T)PDB
        </p>

        <p>
          At that moment, they do get more confused, but here‚Äôs the analogy I use: suppose a student in a Spanish course is given an essay assignment, with a prompt in Spanish (B).
          If the student wants to write the essay in English (D), what do they do?
        </p>

        <p>
          <ul>
            <li>
              <p>
                Take the Spanish prompt (which is our input vector), and translate the prompt to English (that‚Äôs multiplying by PDB).
              </p>
            </li>

            <li>
              <p>
                Write the whole essay in English (that‚Äôs multiplying by MD(T))
              </p>
            </li>

            <li>
              <p>
                Translate the essay back to Spanish (that‚Äôs multiplying by PBD)
              </p>
            </li>
          </ul>
        </p>

        <p>
          Of course, that‚Äôs a terrible idea in reality, both for integrity reasons AND because Spanish/English doesn‚Äôt translate perfectly, BUT mathematically it does work perfectly :)
        </p>

        <p>
          ¬ß9.3 goes really deep into the idea of block diagonal matrices.
          Given the level of this course, I think it‚Äôs better to stick with the notion of diagonalizable linear operators.
          This means we‚Äôre not really following the book in this section after Eigenvalues.
          Please check out my notes for how I guide them through this.
          Alternatively, you could also combine the end of ¬ß9.2 (characteristic equation) with the problem of finding the eigenvalues/eigenvectors of a linear operator, then finding the diagonalization.
        </p>

        <p>
          They want to know why non-standard bases would be used.
          It‚Äôs useful to point out that for a diagonal matrix representation, we can quickly deduce a lot of information about the linear operator (ex: determinant, eigenvalues, invertibility) as well as what happens when we repeatedly apply the linear operator.
        </p>

        <p>
          <em>TA Notes:</em> They‚Äôre going to try to use the standard basis (especially for polynomials) for everything, even if the problem explicitly says to use a different basis, so watch out.
        </p>
      </chapter>


      <chapter xml:id="ch-">
        <title></title>

      </chapter>


      <chapter xml:id="ch-chapter-10-5-3-10-1-10-2-">
        <title>Chapter 10 (¬ß5.3, ¬ß10.1-¬ß10.2)</title>

        <p>
          Learning Goals:
        </p>

        <p>
          <ul>
            <li>
              <p>
                Be able to compute length of vectors in R^2 and R^3
              </p>
            </li>

            <li>
              <p>
                Be able to compute the standard inner product/dot product of vectors
              </p>
            </li>

            <li>
              <p>
                Be familiar with the basic properties of the dot product (including the test for orthogonality
              </p>
            </li>

            <li>
              <p>
                Know how to verify if a function is an inner product
              </p>
            </li>

            <li>
              <p>
                Be able to compute the length of a vector and find a unit vector given an inner product
              </p>
            </li>

            <li>
              <p>
                Be able to find the distance between two vectors using an inner product
              </p>
            </li>

            <li>
              <p>
                Be familiar with the Cauchy‚ÄìBunyakovsky‚ÄìSchwarz (CBS) Inequality and its implications
              </p>
            </li>

            <li>
              <p>
                Know how to check if two vectors are orthogonal given an inner product
              </p>
            </li>

            <li>
              <p>
                Know how to find the coefficients of a linear combination using an orthogonal/orthonormal basis
              </p>
            </li>

            <li>
              <p>
                Be able to build an orthonormal basis given an orthogonal basis
              </p>
            </li>

            <li>
              <p>
                Be able to build an orthogonal basis given any basis using the Gram-Schmidt algorithm (note: not on final exam)
              </p>
            </li>
          </ul>
        </p>

        <p>
          <em>Instructor Notes:</em> ¬ß5.3 covers dot product over ‚Ñùn, and then ¬ß10.1 generalizes to general inner products. Note that this book does not list ‚Äú&#x3C;v,v>=0 iff v=0‚Äù as one of the required inner product axioms; it states it as a consequence. They should be able to verify a function is/is not an inner product, though we‚Äôll give them a specific property to prove/disprove. You should plan to go over the inner product &#x3C;p,q>=01p(x)q(x)dx (for polynomials) with them in class, and practice computing length and distance. They will try to use the dot product for everything.
        </p>

        <p>
          There should be room in the schedule to discuss additional inner products, such as &#x3C;A,B>=tr(ABT) for matrices, evaluation of polynomials, and defining an inner product in terms of a positive definite matrix (however, we did not cover the Chapter 8 in the book, so be sure to define positive definite here).
        </p>

        <p>
          The book states CBS Inequality with squares on everything, but I think it‚Äôs more useful in the unsquared form (using absolute value).
        </p>

        <p>
          They should be prepared to check orthogonality for non-standard inner products.
          They should be able to use an inner product to find coefficients, and be able to build an orthonormal basis from an orthogonal basis.
          Gram-Schmidt is on the homework, but not on the final, so I‚Äôd go over it quickly in class (note: our book doesn‚Äôt do projections until later).
        </p>

        <p>
          <em>TA Notes:</em> Please emphasize computing inner products, length, and distance using weird inner products does NOT give the same answer as the dot product. Vectors which are orthogonal with respect to the dot product may not be orthogonal with respect to another inner product.
        </p>
      </chapter>
    </section>
  </article>
</pretext>